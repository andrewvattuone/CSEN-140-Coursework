{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90012ef-2a76-4680-942b-78165266353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /WAVE/users2/unix/avattuone/.local/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /WAVE/users2/unix/avattuone/.local/lib/python3.9/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /WAVE/users2/unix/avattuone/.local/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /WAVE/users2/unix/avattuone/.local/lib/python3.9/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /WAVE/apps2/el8/conda/envs/JupyterHub/20240807-CPU/lib/python3.9/site-packages (from nltk) (4.66.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /WAVE/users2/unix/avattuone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /WAVE/users2/unix/avattuone/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /WAVE/users2/unix/avattuone/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -m pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eada58e4-02eb-404c-b8a4-b65ab9a100e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data number of features: 3104732\n",
      "Test data number of features: 3104732\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    \"\"\"\n",
    "    Remove punctuation from a string\n",
    "    \"\"\"\n",
    "    return s.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "def add_bigrams_and_trigrams(tokens):\n",
    "    bigrams = ['_'.join(bg) for bg in ngrams(tokens, 2)]\n",
    "    trigrams = ['_'.join(tg) for tg in ngrams(tokens, 3)]\n",
    "    return tokens + bigrams + trigrams\n",
    "\n",
    "def preprocess_line(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [\n",
    "                lemmatizer.lemmatize(stemmer.stem(w.lower())) \n",
    "                for w in remove_punctuation(line).split() \n",
    "                if len(w) > 0 and w.lower() not in stop_words\n",
    "        ] \n",
    "\n",
    "    return add_bigrams_and_trigrams(tokens)\n",
    "\n",
    "def build_matrix(docs, idx, is_training):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    if len(idx) == 0:\n",
    "        tid = 0\n",
    "        nnz = 0\n",
    "        for d in docs:\n",
    "            nnz += len(set(d))\n",
    "            for w in d:\n",
    "                if w not in idx:\n",
    "                    idx[w] = tid\n",
    "                    tid += 1\n",
    "    \n",
    "    else:\n",
    "        nnz = 0\n",
    "        for d in docs:\n",
    "            nnz += len([w for w in set(d) if w in idx])\n",
    "    ncols = len(idx)\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d if is_training else [w for w in d if w in idx])\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "def process_data(fpath, idx=None, is_training=True):\n",
    "    \"\"\"\n",
    "    This function processes the input data and returns the result.\n",
    "    \n",
    "    :param fpath: Input data to be processed\n",
    "    :param idx: Optional dictionary of tokens\n",
    "    :param is_training: Boolean flag indicating if the data is for training\n",
    "    :return: csr_matrix containing the term-frequency values for the input data after processing\n",
    "    \"\"\"\n",
    "     # Example processing: Read the data from fpath, split labels and features (if training data),\n",
    "    # tokenize it, optionally filter tokens, apply stemming or lematization (but not both), \n",
    "    # and finally convert the data to a csr_matrix format by counting token frequencies in each doc.\n",
    "\n",
    "    labels = []\n",
    "    docs = []\n",
    "    \n",
    "    with open(fpath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if is_training:\n",
    "                labels.append(line[0])\n",
    "                line = line[2:]\n",
    "            processed = preprocess_line(line)\n",
    "            docs.append(processed)\n",
    "\n",
    "    data = build_matrix(docs, idx, is_training)\n",
    "    csr_l2normalize(data)\n",
    "   \n",
    "    return (data, labels) if is_training else data\n",
    "    \n",
    "# Example usage\n",
    "idx = {}\n",
    "train_data, train_labels = process_data('train.dat', idx=idx, is_training=True)\n",
    "test_data = process_data('test.dat', idx=idx, is_training=False)\n",
    "print(f\"Training data number of features: {train_data.shape[1]}\")\n",
    "print(f\"Test data number of features: {test_data.shape[1]}\")\n",
    "assert train_data.shape[1] == test_data.shape[1], \"Train and test data do not have the same number of features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a85ce7-6309-4a06-ae31-c8284ba0a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 11 x_proximities out of 5 using proximity function: [(62678, 0.2533954906327426), (16712, 0.22782391365303153), (30610, 0.2231436834155872), (45692, 0.2136327344609569), (97924, 0.20689655172413782)]\n",
      "Top 11 y_proximities out of 5 using proximity function: [(37958, 0.33456448556986007), (36323, 0.21609716071294321), (85565, 0.2140712823141403), (89430, 0.21200662374236884), (71431, 0.20567663600479233)]\n",
      "Top 11 z_proximities out of 5 using proximity function: [(25407, 0.5225157910494113), (15788, 0.39478625833506414), (50542, 0.3522360916804568), (23060, 0.3428480345893351), (60195, 0.3346838149756099)]\n",
      "Top 11 a_proximities out of 5 using proximity function: [(52162, 0.18156825980064084), (14894, 0.17777777777777784), (22178, 0.16169041669088854), (88116, 0.14611900125320162), (90448, 0.1454785934906617)]\n"
     ]
    }
   ],
   "source": [
    "def calc_proximities(train_data, x, k=5):\n",
    "    \"\"\"\n",
    "    This function computes the proximity of a given data point x to the training data.\n",
    "    \n",
    "    :param train_data: The training data in csr_matrix format\n",
    "    :param x: The data point for which proximity is to be computed\n",
    "    :return: list of pairs containing the rows in `train_data` that have non-zero proximity to the sample and their associated proximity values\n",
    "    \"\"\"\n",
    "    dots = x.dot(train_data.T)\n",
    "    proximities = [(int(idx), float(val)) for idx, val in zip(dots.indices, dots.data)]\n",
    "    sorted_proximities = sorted(proximities, key=lambda x: x[1], reverse=True)\n",
    "    top_k_proximities = sorted_proximities[:k]\n",
    "    \n",
    "    return top_k_proximities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8e1b4d-99e2-4043-9029-b5920a651f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(train_data, train_labels, test_data):\n",
    "    with open(\"predictions.dat\", \"w\") as file:\n",
    "        for line in test_data:\n",
    "            proximities = calc_proximities(train_data, line)\n",
    "            potential_labels = {}\n",
    "            \n",
    "            for proximity in proximities:\n",
    "                proximity_label = train_labels[proximity[0]]\n",
    "                \n",
    "                if proximity_label not in potential_labels:\n",
    "                    potential_labels[proximity_label] = 0.0\n",
    "                potential_labels[proximity_label] += proximity[1]\n",
    "                \n",
    "            majority_label = max(potential_labels, key=potential_labels.get)\n",
    "            file.write(f'{majority_label}\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ecfef-0479-4389-9a6b-7856091a70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels(train_data, train_labels, test_data)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41541fb4-1d24-42b0-8875-142a810c3c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "140wi25",
   "language": "python",
   "name": "140wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

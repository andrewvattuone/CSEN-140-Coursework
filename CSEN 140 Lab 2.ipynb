{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Text Processing\n",
    "\n",
    "In this lab, you will pre-process the text data from your Program 1 assignment and practice computing at least two proximity measures using the sparse vectors you create from your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Read in the data from train.dat and test.dat, split labels from text in the training data, and process the text in the training and test datasets into sparse matrices `train_data` and `test_data`. You may use and modify the code provided for you in the `Activity-data-3` example or use external libraries for this task. Note that both the training and test data must be in the same Euclidean space, i.e., each axis of the space should refer to the same token/word and there should be the same number of columns in the `train_data` and `test_data` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m pip install nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data number of features: 74290\n",
      "Test data number of features: 74290\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    \"\"\"\n",
    "    Remove punctuation from a string\n",
    "    \"\"\"\n",
    "    return s.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "def preprocess_line(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [\n",
    "                lemmatizer.lemmatize(stemmer.stem(w.lower())) \n",
    "                for w in remove_punctuation(line).split() \n",
    "                if len(w) > 0 and w.lower() not in stop_words\n",
    "        ] \n",
    "\n",
    "def build_matrix(docs, idx, is_training):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    if len(idx) == 0:\n",
    "        tid = 0\n",
    "        nnz = 0\n",
    "        for d in docs:\n",
    "            nnz += len(set(d))\n",
    "            for w in d:\n",
    "                if w not in idx:\n",
    "                    idx[w] = tid\n",
    "                    tid += 1\n",
    "    \n",
    "    else:\n",
    "        nnz = 0\n",
    "        for d in docs:\n",
    "            nnz += len([w for w in set(d) if w in idx])\n",
    "    ncols = len(idx)\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d if is_training else [w for w in d if w in idx])\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def process_data(fpath, idx=None, is_training=True):\n",
    "    \"\"\"\n",
    "    This function processes the input data and returns the result.\n",
    "    \n",
    "    :param fpath: Input data to be processed\n",
    "    :param idx: Optional dictionary of tokens\n",
    "    :param is_training: Boolean flag indicating if the data is for training\n",
    "    :return: csr_matrix containing the term-frequency values for the input data after processing\n",
    "    \"\"\"\n",
    "     # Example processing: Read the data from fpath, split labels and features (if training data),\n",
    "    # tokenize it, optionally filter tokens, apply stemming or lematization (but not both), \n",
    "    # and finally convert the data to a csr_matrix format by counting token frequencies in each doc.\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    with open(fpath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for l in lines:\n",
    "            if is_training:\n",
    "                l = l[2:]\n",
    "            processed = preprocess_line(l)\n",
    "            docs.append(processed)\n",
    "\n",
    "    data = build_matrix(docs, idx, is_training)\n",
    "   \n",
    "    return data\n",
    "    \n",
    "# Example usage\n",
    "idx = {}\n",
    "train_data = process_data('train.dat', idx=idx, is_training=True)\n",
    "test_data = process_data('test.dat', idx=idx, is_training=False)\n",
    "print(f\"Training data number of features: {train_data.shape[1]}\")\n",
    "print(f\"Test data number of features: {test_data.shape[1]}\")\n",
    "assert train_data.shape[1] == test_data.shape[1], \"Train and test data do not have the same number of features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Now that you have the data, create efficient methods to compute at least one proximity function for samples in your data. In particular, to solve the k-NN problem you need to find the nearest neighbors within the training data (rows of `train_data`) for each of the test data points (rows of `test_data`). You may choose any proximity measures you would like. However, there are several requirements for this assignment:\n",
    "1. You may not use any external libraries other than functions inherent in the `csr_matrix` data structure.\n",
    "2. Your data matrix/vectors should not be changed from sparse to dense at any time during the computation.\n",
    "3. You should return a list of pairs containing the rows in `train_data` that have non-zero proximity to the sample and their associated proximity values.\n",
    "\n",
    "For best performance, computations should be vectorized whenever possible, i.e., you should compute vector-matrix or matrix-matrix operations rather than vector-vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 proximities out of 102080 using proximity function: [(62678, 0.42874646285627205), (45692, 0.4117647058823529), (97924, 0.4115966043420212)]\n"
     ]
    }
   ],
   "source": [
    "def sparse_dot_product(row, x):\n",
    "    x_i, x_v = x.indices, x.data\n",
    "    r_i, r_v = row.indices, row.data\n",
    "\n",
    "    dot = 0.0\n",
    "    i = j = 0\n",
    "\n",
    "    while i < len(x_i) and j < len(r_i):\n",
    "        if x_i[i] == r_i[j]:\n",
    "            dot += x_v[i] * r_v[j]\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif x_i[i] < r_i[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return dot\n",
    "\n",
    "\n",
    "def proximity(train_data, x):\n",
    "    \"\"\"\n",
    "    This function computes the proximity of a given data point x to the training data.\n",
    "    \n",
    "    :param train_data: The training data in csr_matrix format\n",
    "    :param x: The data point for which proximity is to be computed\n",
    "    :return: list of pairs containing the rows in `train_data` that have non-zero proximity to the sample and their associated proximity values\n",
    "    \"\"\"\n",
    "    proximities = []\n",
    "    x_mag = sum(val * val for val in x.data) ** 0.5\n",
    "    for i in range(train_data.shape[0]):\n",
    "        row = train_data.getrow(i)\n",
    "        dot_product = sparse_dot_product(row, x)\n",
    "        row_mag = sum(val * val for val in row.data) ** 0.5\n",
    "\n",
    "        if row_mag > 0 and x_mag > 0:\n",
    "            sim = dot_product / (row_mag * x_mag)\n",
    "            # each proximity is a pair, with the first value being the row number of the proximity and the second value the actual proximity\n",
    "            proximities.append((i, float(sim)))\n",
    "            \n",
    "    proximities.sort(key=lambda pair: pair[1], reverse=True)\n",
    "    return proximities\n",
    "\n",
    "# Example usage \n",
    "x = test_data[0]\n",
    "proximities = proximity(train_data, x)\n",
    "print(f\"Top 3 proximities out of {len(proximities)} using proximity function:\", proximities[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "140wi25",
   "language": "python",
   "name": "140wi25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
